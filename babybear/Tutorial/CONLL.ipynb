{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc0b73a",
   "metadata": {},
   "source": [
    "## Conll2003 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6da0d",
   "metadata": {},
   "source": [
    "This notebook is to generate the results on [conll2003 dataset](https://huggingface.co/datasets/conll2003) taken from hugging face. The model in this work is [dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f903f0b",
   "metadata": {},
   "source": [
    "#### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import numpy as np\n",
    "import pickle5 as pkl\n",
    "import tensorflow_hub as hub\n",
    "import util_funcs as uf\n",
    "from nlx_babybear import RFBabyBear\n",
    "from inference_triage import PapabearClassifier, TriagedClassifier\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683982b",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/conll2003/train_conll.pkl'\n",
    "texts_train, y_train, ner_train = uf.open_pkl(filename)\n",
    "doc, labels = np.asarray(texts_train), np.asarray(y_train)\n",
    "\n",
    "filename = '../data/conll2003/test_conll.pkl'\n",
    "texts_test, y_test, ner_test = uf.open_pkl(filename)\n",
    "texts_test, y_test = np.asarray(texts_test), np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae2194",
   "metadata": {},
   "source": [
    "There are 2 classes in this dataset, 0 and 1. 0 shows the sentences without any entities and 1 shows the sentences with entities in it.  The distribution of these classes in the training dataset is shown in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = np.where(labels == 0)[0]\n",
    "label_1 = np.where(labels == 1)[0]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['Sentences with NO entity', 'Sentences WITH entity']\n",
    "students = [len(label_0),len(label_1)]\n",
    "ax.bar(langs,students)\n",
    "plt.ylabel('# of sentences in all dataset')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9982a3",
   "metadata": {},
   "source": [
    "A histogram of number of tokens in each document on test dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ecd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for i in range(len(ner_test)):\n",
    "    length.append(len(ner_test[i]))\n",
    "\n",
    "plt.hist(length, bins=max(length))\n",
    "plt.xlabel('Number of entities in sentence level', fontsize=18)\n",
    "plt.ylabel('Frequency',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8f5c4",
   "metadata": {},
   "source": [
    "#### Input file:\n",
    "\n",
    "`model`: The model used as [papabear model]((https://huggingface.co/dslim/bert-base-NER))\n",
    "\n",
    "`confidence_th_options`: The values for confidence threshold\n",
    "\n",
    "`metric`: The metric to find the performance. It can be one of the \"accuracy\", \"recall\", \"f1_score\" and \"precision\".\n",
    "\n",
    "`metric_threshold`: The minimum value of performance we are expecting for the final model to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "confidence_th_options = np.arange(0,1.005,.005)\n",
    "metric = \"accuracy\"\n",
    "metric_threshold = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8bc4b",
   "metadata": {},
   "source": [
    "#### Instantiate babybear and papbear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f61d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "papabear = PapabearClassifier(model, tokenizer)\n",
    "babybear = RFBabyBear(language_model)\n",
    "\n",
    "inf_traige = TriagedClassifier(\"ner\", babybear, papabear, metric_threshold, \"accuracy\", confidence_th_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98ecc5",
   "metadata": {},
   "source": [
    "#### hyper-parameter tuning\n",
    "\n",
    "Here we will train inference triage to find the confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432814bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_traige.train(doc, labels)\n",
    "\n",
    "print(f\"Confidence threshold is: {inf_traige.confidence_th}\")\n",
    "\n",
    "print(f\"The following plots are the saving vs Threshold for different CV fold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe8060",
   "metadata": {},
   "source": [
    "#### Training babybear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "babybear = RFBabyBear(language_model)\n",
    "babybear.train(doc, labels, n_class=len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71774fd",
   "metadata": {},
   "source": [
    "#### Applying inference triage on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_traige.babybear = babybear\n",
    "a = inf_traige.score(texts_test, y_test)\n",
    "\n",
    "dump_data = {}\n",
    "dump_data['result'] = a\n",
    "dump_data['confidence_th'] = inf_traige.confidence_th\n",
    "dump_data['indx_conf_th'] = inf_traige.indx_conf_th\n",
    "dump_data['metric'] = inf_traige.metric\n",
    "dump_data['metric_threshold'] = inf_traige.metric_threshold\n",
    "dump_data['performance'] = inf_traige.performance\n",
    "dump_data['saving'] = inf_traige.saving\n",
    "dump_data['tot_time'] = inf_traige.tot_time\n",
    "\n",
    "with open('../output/conll.resullts', 'wb') as outp:  # Overwrites any existing file.\n",
    "        pkl.dump(dump_data, outp, pkl.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b207f0",
   "metadata": {},
   "source": [
    "#### Plot cpu/gpu run time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_traige['tot_time'], np.asarray(inf_traige['performance'])*100, color='r', label='GPU run time')\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel(str(inf_traige['metric']))\n",
    "\n",
    "y = np.arange(0, 105, .1)\n",
    "x = y * 0 + inf_traige['tot_time'][inf_traige['indx_conf_th']]\n",
    "plt.plot(x, y, '--', label='accuracy at confidence threshold =' + str(str(inf_traige['performance'][inf_traige['indx_conf_th']]*100)) + '%')\n",
    "plt.ylim([min(inf_traige['performance'])*100-5, 105])\n",
    "\n",
    "x = np.arange(-.5, max(inf_traige['tot_time'])+.5, .1)\n",
    "y = x * 0 + inf_traige['performance'][inf_traige['indx_conf_th']]*100\n",
    "plt.plot(x, y, '--', label='Time at confidence_th')\n",
    "plt.xlim([-.1, max(inf_traige['tot_time'])+.5])\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65d296",
   "metadata": {},
   "source": [
    "Saving vs confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fef156",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(0,1.005,.005),inf_traige['saving'], color='r', label='GPU run time')\n",
    "plt.xlabel('confidence threshol')\n",
    "plt.ylabel('saving')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb12b44",
   "metadata": {},
   "source": [
    "Performance vs confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443affa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(0,1.005,.005),inf_traige['performance'], color='r', label='GPU run time')\n",
    "plt.xlabel('confidence threshol')\n",
    "plt.ylabel(str(inf_traige['metric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c14870",
   "metadata": {},
   "source": [
    "Gpu run time vs confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(0,1.005,.005),inf_traige['tot_time'], color='r', label='GPU run time')\n",
    "plt.xlabel('confidence threshold')\n",
    "plt.ylabel('time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
